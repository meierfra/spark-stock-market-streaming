{"cells":[{"cell_type":"markdown","source":["# Tweet Consumer\n\n@author: @raymondmarfurt\n\nStreaming with Spark<br>\nZHAW CAS Machine Intelligence<br>\nBig Data Project<br>\n\nConsume JSON files produced by Twitter Producer. Apply windowing and aggregate by weighted sum of tweets per hashtag."],"metadata":{}},{"cell_type":"code","source":["HASHTAGS = ['#tesla',\n            '#apple',\n            '#Microsoft',\n            '#mcdonalds',\n            '#nike',\n            '#pfizer',\n            '#facebook',\n            '#alphabet',\n            '#goldmansachs',\n            '#lockheadmartin']\n\nHASHTAG_STR = \"'tesla', 'apple', 'Microsoft', 'mcdonalds', 'nike', 'pfizer', 'facebook', 'alphabet', 'goldmansachs', 'lockheadmartin'\"\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["def printDF(streamingDF):\n  iter = 0\n  while iter < 50:\n    print(\"testing for streamingDF.count()...\")\n    if(streamingDF.count() > 0):\n      print(\"Number of entries in dataframe: \"+ str(streamingDF.count()))\n      streamingDF.select(\"hashtag\", \"end\", \"weight\").toPandas().to_csv('/tmp/statistics')\n      streamingDF.show(20, False) # the parameter False prevents Spark from truncating the output\n      iter += 1\n    time.sleep(60)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["df = spark.read.json(\"/tmp/tweets/data_twitter.json\")\njson_schema = df.schema\ndf.printSchema()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.functions import *\nimport time\n\nTWEET_DIR = \"/tmp/tweet_small/\"\n\nstreamingInputDF = (\n  spark\n    .readStream\n    .schema(json_schema)\n    .json(TWEET_DIR)\n)\n\n# You can check if this is streaming dataframe using the following:\nstreamingInputDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["dbutils.fs.ls(TWEET_DIR)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Group the data by window and hashtag and compute the count of each group\nimport math\nimport pyspark.sql.functions as sf\n\nw1 = streamingInputDF.withColumn(\"timestamp\", to_timestamp(\"created_at\", \"EEE MMM dd HH:mm:ss '+0000' yyyy\"))\n#w2 = w1.withColumn(\"log_followers_count\", math.log(int(w1['user']['followers_count'])))\nw3 = w1.withWatermark('timestamp', \"1 minute\")\nwindowedCounts = w3.select(\"id\", \"user.followers_count\", explode(streamingInputDF.entities.hashtags.text).alias(\"hashtag\"), \"timestamp\")\n\n\ngroupedCounts = windowedCounts.groupBy(\n  windowedCounts.hashtag, \n  window(windowedCounts.timestamp, \"1 minute\")\n).agg(sf.sum(\"followers_count\").alias(\"weight\"))#.orderBy('window')\n\nstreamingETLQuery = groupedCounts \\\n  .writeStream \\\n  .format(\"memory\") \\\n  .queryName(\"tweetstream\") \\\n  .outputMode(\"append\")\\\n  .start()\n  \n\nstreamingDF = spark.sql(\"select hashtag, window.end, weight  from tweetstream \")\n#streamingDF = spark.sql(\"select * from tweetstream\")\n#streamingDF = spark.sql(\"select count(*) as count, window, lower(hashtag) as hashtag from tweetstream where lower(hashtag) in ('tesla', 'apple', 'Microsoft', 'mcdonalds', 'nike', 'pfizer', 'facebook', 'alphabet', 'goldmansachs', 'lockheadmartin') group by window, hashtag order by count desc\")\n\nprintDF(streamingDF)\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Finally, stop the running streaming query\nstreamingETLQuery.stop()"],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"twitter_consumer","notebookId":3387075430854459},"nbformat":4,"nbformat_minor":0}
