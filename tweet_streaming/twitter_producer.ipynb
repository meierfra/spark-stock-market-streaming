{"cells":[{"cell_type":"markdown","source":["# Tweet Producer\n\n@author: @raymondmarfurt\n\nStreaming with Spark<br>\nZHAW CAS Machine Intelligence<br>\nBig Data Project<br>\n\nUse Twitter realtime API to collect tweets by filtering for specific hashtags, consolidate hashtags and store as raw JSON files"],"metadata":{}},{"cell_type":"code","source":["import tweepy\nimport json\nimport time\nimport traceback\n\nCONSUMER_KEY = 'xxx'\nCONSUMER_SECRET = 'yyy'\nOAUTH_TOKEN = 'aaa'\nOAUTH_TOKEN_SECRET = 'bbb'\n\nHASHTAGS = ['#tesla',\n            '#apple',\n            '#Microsoft',\n            '#mcdonalds',\n            '#nike',\n            '#pfizer',\n            '#facebook',\n            '#alphabet',\n            '#goldmansachs',\n            '#lockheadmartin']\nHASHTAGS2 = [h[1:].lower() for h in HASHTAGS]\n\nTWEET_DIR = \"/tmp/tweet_small/\"\n\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["dbutils.fs.rm(TWEET_DIR, recurse=True)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["dbutils.fs.mkdirs(TWEET_DIR)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["dbutils.fs.ls(TWEET_DIR)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import os\nFILE_MAX_THRESHOLD = 40\nFILE_MIN_THRESHOLD = 30\nfile_list = os.listdir(\"/dbfs\" + TWEET_DIR)\nnumber_of_files = len(file_list)\n\nprint(\"Number of files: \" + str(number_of_files))\n\ndiff = 0\n\nif number_of_files < FILE_MAX_THRESHOLD:\n  print(\"Nothing to cleanup\")\nelse:\n  diff = number_of_files-FILE_MIN_THRESHOLD\n\nfor f in sorted(file_list)[:diff]:\n  print(\"deleting file \" + f)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["import os\nimport json\nimport sys\nimport traceback\n\nclass MyStreamListener(tweepy.StreamListener):\n    FILE_MAX_THRESHOLD = 40\n    FILE_MIN_THRESHOLD = 30\n\n    def on_status(self, status):\n        id = status.id\n        print(\"Processing tweet\" + str(id))\n        file_name = \"/dbfs\" + TWEET_DIR + str(id)\n        new_status = self._transform_ht(status)\n        if not new_status._json['entities']['hashtags'] == None:\n            with open(file_name, 'w') as f:\n                json.dump(new_status._json, f)\n        else:\n            print(\"skipping empty hashtag\")\n        self._cleanup()\n\n    def on_error(self, status_code):\n        if status_code == 420:\n            print(\"!!! encountered 420. Aborting.\")\n            return False\n        else:\n            print(\"!!! UNKNOWN ERROR \" + str(status_code))\n            return False\n          \n    def _cleanup(self):\n        file_list = os.listdir(\"/dbfs\" + TWEET_DIR)\n        number_of_files = len(file_list)\n        print(\"Number of files in output folder: \" + str(number_of_files))\n\n        diff = 0\n        if number_of_files > self.FILE_MAX_THRESHOLD:\n            diff = number_of_files-self.FILE_MIN_THRESHOLD\n            print(\"Need to clean up. Deleting \" + str(diff) + \" files...\")\n            for f in sorted(file_list)[:diff]:\n                dbutils.fs.rm(TWEET_DIR + f)\n              \n              \n    def _transform_ht(self, status):\n        tweet = status._json\n        retval = {}\n        hashtag_list = []\n        ht = None\n        \n        ht = self._check_for_hashtag(tweet['entities']['hashtags'])\n        if ht == None and 'extended_tweet' in tweet.keys():\n            ht = self._check_for_hashtag(tweet['extended_tweet']['entities']['hashtags'])\n        if ht == None and 'retweeted_status' in tweet.keys():\n            ht = self._check_for_hashtag(tweet['retweeted_status']['entities']['hashtags'])\n            if ht == None and 'extended_tweet' in tweet['retweeted_status'].keys()\\\n                and 'entities' in tweet['retweeted_status']['extended_tweet'].keys()\\\n                and 'hashtags' in tweet['retweeted_status']['extended_tweet']['entities'].keys():\n                ht = self._check_for_hashtag(tweet['retweeted_status']['extended_tweet']['entities']['hashtags'])\n        if ht == None and 'quoted_status' in tweet.keys():\n            ht = self._check_for_hashtag(tweet['quoted_status']['entities']['hashtags'])\n            if ht == None and 'extended_tweet' in tweet['quoted_status'].keys()\\\n                and 'entities' in tweet['quoted_status']['extended_tweet'].keys()\\\n                and 'hashtags' in tweet['quoted_status']['extended_tweet']['entities'].keys():\n                ht = self._check_for_hashtag(tweet['quoted_status']['extended_tweet']['entities']['hashtags'])\n                \n        if ht != None:\n          ht['text'] = ht['text'].lower()\n          print(\"setting hashtag to \" + ht['text'])\n        tweet['entities']['hashtags'] = [ht]\n        return status\n\n    def _check_for_hashtag(self, ht_list):\n        for ht in ht_list:\n            if ht['text'].lower() in HASHTAGS2:\n                return ht\n        return None\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\nauth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n\nmyStreamListener = MyStreamListener()\nlast_try_time = 0\n\nwhile True:\n    print(\"---- start fetching ----\")\n    current_time = time.time()\n    if current_time-last_try_time < 60*15:\n        print(\"retrying too fast. Sleeping for 15min.\")\n        time.sleep(60*15)\n    try:\n        myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n        myStream.filter(track=HASHTAGS, async=False)\n    except IOError as ex:\n        print('I just caught the exception: %s' % ex)\n        traceback.print_stack()\n        time.sleep(60)\n\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#dbutils.fs.rm(\"/tmp/tweet_data\", recurse=True)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"twitter_producer","notebookId":1528342496491173},"nbformat":4,"nbformat_minor":0}
