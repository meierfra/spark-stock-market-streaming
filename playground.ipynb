{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground Notebook\n",
    "## To try, validate and debug the code into the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock Names\n",
    "stock_names = {'GS':'Goldman Sachs Group Inc',\n",
    "               'LMT':'Lockheed Martin Corporation',\n",
    "               'TSLA':'TESLA',\n",
    "               'MSFT':'Microsoft Corporation',\n",
    "               'AAPL':'Apple Inc.',\n",
    "               'MCD':'McDonalds Corporation',\n",
    "               'NKE':'Nike Inc',\n",
    "               'PFE':'Pfizer Inc.',\n",
    "               'FB':'Facebook, Inc.',\n",
    "               'GOOGL':'Alphabet Inc.'\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of timestamp to parse on the raw data file\n",
    "stamp_name = '4. timestamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names on the raw data file\n",
    "header_names = {'1. symbol':'sym',\n",
    "                '2. price':'price_str', \n",
    "                '3. volume':'vol'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories and Files\n",
    "# Windows\n",
    "if os.name == 'nt':\n",
    "    home_dir = r'//userhome/users$/ksagilop/home/ZHAW/MAIN/04_Big_Data/'\n",
    "    stock_dir = r'spark-stock-market-streaming/collected_data/'\n",
    "    tweet_dir = r'spark-stock-market-streaming/collected_tweets_csv_raw/'\n",
    "# Linux\n",
    "if os.name == 'posix':\n",
    "    home_dir = os.path.expanduser(r'~/Documents/ZHAW/MAIN/04_Big_Data/30_Project/')\n",
    "    stock_dir = r'spark-stock-market-streaming/collected_data/'\n",
    "    tweet_dir = r'spark-stock-market-streaming/collected_tweets_csv_raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataframe from json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDF(base_dir, data_dir, json_col):\n",
    "    '''Construct a big dataframe by aggregating the individual json files\n",
    "    located at the proper data directory\n",
    "    Args:\n",
    "        base_dir(str), the home or base directory\n",
    "        data_dir(str), the directory containing the data\n",
    "        json_col(str), which column to normalize from the json file\n",
    "    return:\n",
    "        df(dataframe), full dataframe iaw json structure'''\n",
    "    folder = os.path.join(base_dir + data_dir)\n",
    "    files = os.listdir(folder)\n",
    "    count_files = 0\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder + file)\n",
    "        with open(file_path) as data_file:\n",
    "            data = json.load(data_file)\n",
    "            if count_files == 0:\n",
    "                df = json_normalize(data, json_col)\n",
    "                print('---- Base Dataframe ----')\n",
    "                print('Lenght of base dataframe is: ', len(df))\n",
    "                print(file_path)\n",
    "                print(df.head())\n",
    "                count_files += 1\n",
    "            else:\n",
    "                try:\n",
    "                    df_temp = json_normalize(data, json_col)\n",
    "                    df = df.append(df_temp, ignore_index=True)\n",
    "                    count_files += 1\n",
    "                except:\n",
    "                    print(file_path, 'normalize failed')\n",
    "                    continue\n",
    "    print('------------------------')\n",
    "    print('Total files read: ' + str(count_files))\n",
    "    print('---- %d Dataframes appended ----' %count_files)\n",
    "    print('Total lenght of dataframe is: ', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Base Dataframe ----\n",
      "Lenght of base dataframe is:  10\n",
      "//userhome/users$/ksagilop/home/ZHAW/MAIN/04_Big_Data/spark-stock-market-streaming/collected_data/20180606-140045.json\n",
      "  1. symbol  2. price 3. volume         4. timestamp\n",
      "0      TSLA  290.9400        --  2018-06-05 15:59:55\n",
      "1      AAPL  193.3600        --  2018-06-05 15:59:57\n",
      "2      MSFT  102.2100        --  2018-06-05 15:59:55\n",
      "3       MCD  159.4800        --  2018-06-05 15:59:57\n",
      "4       NKE   74.0600        --  2018-06-05 15:59:57\n",
      "//userhome/users$/ksagilop/home/ZHAW/MAIN/04_Big_Data/spark-stock-market-streaming/collected_data/20180614-173053.json normalize failed\n",
      "//userhome/users$/ksagilop/home/ZHAW/MAIN/04_Big_Data/spark-stock-market-streaming/collected_data/20180614-175150.json normalize failed\n",
      "//userhome/users$/ksagilop/home/ZHAW/MAIN/04_Big_Data/spark-stock-market-streaming/collected_data/20180614-180447.json normalize failed\n",
      "------------------------\n",
      "Total files read: 3149\n",
      "---- 3149 Dataframes appended ----\n",
      "Total lenght of dataframe is:  31490\n"
     ]
    }
   ],
   "source": [
    "dfs = buildDF(home_dir, stock_dir, 'Stock Quotes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transDF(df, stamp, header, names):\n",
    "    '''Pre-processing of the data by doing some transformations\n",
    "    and aggregations to the dataframe created previously\n",
    "    Args:\n",
    "        df(dataframe): input dataframe\n",
    "        stamp(str): name column to parse for datetime\n",
    "        header(dict): a dictionary with {'original col name':'new col name'}\n",
    "        names(dict): a dictionary with the mapping {'stock abrev.':'stock full name'}\n",
    "    return:\n",
    "        df(dataframe): transformed dataframe'''\n",
    "    print('---- Input Dataframe ----')\n",
    "    print(df.head())\n",
    "    # Parse date and time\n",
    "    print('------------------------')\n",
    "    print('Parsing datetimes...')\n",
    "    df['stamp'] = pd.to_datetime(df[stamp])\n",
    "    df['date'] = df['stamp'].dt.date\n",
    "    #df['year'] = df['stamp'].dt.year\n",
    "    #df['month'] = df['stamp'].dt.strftime('%b')\n",
    "    #df['day'] = df['stamp'].dt.day\n",
    "    df['time'] = df['stamp'].dt.time\n",
    "    #print('Dropping auxiliary columns...')\n",
    "    df.drop([stamp], axis=1, inplace=True) # Not needed as 'stamp' is used for x-axis\n",
    "    # Set dataframe index\n",
    "    df.set_index(['date', 'time'], inplace=True)\n",
    "    #df.set_index(['year', 'month', 'day', 'time'], inplace=True)\n",
    "    print('Multi-index set from: ', stamp)\n",
    "    print('Renaming columns...')\n",
    "    df.rename(columns=header, inplace=True)\n",
    "    #print(df.head())\n",
    "    print('Change dtype for price...')\n",
    "    df['price'] = pd.to_numeric(df['price_str'])\n",
    "    df.drop(['price_str'], axis=1, inplace=True)\n",
    "    print('Drop duplicates...')\n",
    "    original_len = len(df)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    drop_len = len(df)\n",
    "    print('Dataframe reduced from %d to %d rows' %(original_len, drop_len))\n",
    "    df.sort_index(inplace=True)\n",
    "    print('Renaming rows...')\n",
    "    for key, value in names.items():\n",
    "        mask = df.sym == key\n",
    "        df.loc[mask, 'sym'] = value + ', ' + '(' + key + ')'\n",
    "    print('---- Modified Dataframe ----')\n",
    "    print(df.head())\n",
    "    print(df.dtypes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Input Dataframe ----\n",
      "  1. symbol  2. price 3. volume         4. timestamp\n",
      "0      TSLA  290.9400        --  2018-06-05 15:59:55\n",
      "1      AAPL  193.3600        --  2018-06-05 15:59:57\n",
      "2      MSFT  102.2100        --  2018-06-05 15:59:55\n",
      "3       MCD  159.4800        --  2018-06-05 15:59:57\n",
      "4       NKE   74.0600        --  2018-06-05 15:59:57\n",
      "------------------------\n",
      "Parsing datetimes...\n",
      "Multi-index set from:  4. timestamp\n",
      "Renaming columns...\n",
      "Change dtype for price...\n",
      "Drop duplicates...\n",
      "Dataframe reduced from 31490 to 22335 rows\n",
      "Renaming rows...\n",
      "---- Modified Dataframe ----\n",
      "                                                    sym vol  \\\n",
      "date       time                                               \n",
      "2018-06-05 15:59:46       Goldman Sachs Group Inc, (GS)  --   \n",
      "           15:59:53  Lockheed Martin Corporation, (LMT)  --   \n",
      "           15:59:55                       TESLA, (TSLA)  --   \n",
      "           15:59:55       Microsoft Corporation, (MSFT)  --   \n",
      "           15:59:57                  Apple Inc., (AAPL)  --   \n",
      "\n",
      "                                  stamp   price  \n",
      "date       time                                  \n",
      "2018-06-05 15:59:46 2018-06-05 15:59:46  228.30  \n",
      "           15:59:53 2018-06-05 15:59:53  320.13  \n",
      "           15:59:55 2018-06-05 15:59:55  290.94  \n",
      "           15:59:55 2018-06-05 15:59:55  102.21  \n",
      "           15:59:57 2018-06-05 15:59:57  193.36  \n",
      "sym              object\n",
      "vol              object\n",
      "stamp    datetime64[ns]\n",
      "price           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "dfss = transDF(dfs, stamp_name, header_names, stock_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfss.price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_hr_min = dfss.index.get_level_values('time').min()\n",
    "s_hr_max = dfss.index.get_level_values('time').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_hr_min, s_hr_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_map = {'nike':'Nike Inc, (NKE)', \n",
    "               'facebook':'Facebook, Inc., (FB)', \n",
    "               'apple':'Apple Inc., (AAPL)', \n",
    "               'microsoft':'Microsoft Corporation, (MSFT)', \n",
    "               'mcdonalds':'McDonalds Corporation, (MCD)', \n",
    "               'tesla':'TESLA, (TSLA)',\n",
    "               'n/a':'NA', \n",
    "               'goldmansachs':'Goldman Sachs Group Inc, (GS)', \n",
    "               'alphabet':'Alphabet Inc., (GOOGL)', \n",
    "               'pfizer':'Pfizer Inc., (PFE)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDFbis(base_dir, data_dir):\n",
    "    '''Construct a big dataframe by aggregating the individual csv files\n",
    "    located at the proper data directory\n",
    "    Args:\n",
    "        base_dir(str), the home or base directory\n",
    "        data_dir(str), the directory containing the data\n",
    "    return:\n",
    "        df(dataframe), full dataframe iaw csv structure'''\n",
    "    folder = os.path.join(base_dir + data_dir)\n",
    "    files = os.listdir(folder)\n",
    "    file_path = os.path.join(folder + files[0])\n",
    "    # Dataframe\n",
    "    df = pd.read_csv(file_path,\n",
    "                     sep=';',\n",
    "                     parse_dates={'stamp':['timestamp']},\n",
    "                     keep_date_col=False)\n",
    "    print('Dataframe created from: ', file_path)\n",
    "    print('with length: ', len(df))\n",
    "    print('--- Dataframe processing ---')\n",
    "    print('Creating date and time columns from timestamp')\n",
    "    df['date'] = df['stamp'].dt.date\n",
    "    df['time'] = df['stamp'].dt.time\n",
    "    print('Setting date and time as Index...')\n",
    "    df.set_index(['date', 'time'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = buildDFbis(home_dir, tweet_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice tweet dataframe to an equal stock timestamp\n",
    "stock_hr_min = dfss.index.get_level_values('time').min()\n",
    "stock_hr_max = dfss.index.get_level_values('time').max()\n",
    "tweet_hr_min = dft.index.get_level_values('time').min()\n",
    "tweet_hr_max = dft.index.get_level_values('time').max()\n",
    "mask_hr_min = dft.index.get_level_values('time') > stock_hr_min\n",
    "dft = dft.loc[mask_hr_min]\n",
    "mask_hr_max = dft.index.get_level_values('time') < stock_hr_max\n",
    "dft = dft.loc[mask_hr_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['stamp_round'] = dft.stamp.dt.round('1Min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = dft.groupby(['stamp_round'])['log_followers_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.groupby(['stamp', 'time_min'])['log_followers_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
